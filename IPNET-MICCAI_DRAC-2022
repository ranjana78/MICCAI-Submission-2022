#import packages
#import audtorch
import scipy
import datetime
import time
from sklearn.preprocessing import normalize
#import tensorflow as tf
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score
from iteration_utilities import flatten
from geopy.distance import geodesic
import csv
import os
import cv2
import sys
from sklearn import metrics
import shutil
import random
import math
import PIL
import pickle
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from PIL import Image
from numpy import asarray
from numpy import array
from numpy import linalg as LA
from collections import Counter
from numpy import dot
from numpy.linalg import norm
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import transforms
from functools import reduce
import json

from itertools import combinations
import matplotlib.pyplot as plt
#from tensorflow.keras.utils import ImageDataGenerator,array_to_img, img_to_array, load_img
#from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img
#from keras.utils.image_utils import array_to_img, img_to_array, load_img
#from keras.preprocessing.image import ImageDataGenerator
#from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img

import torch.nn as nn
from torch.optim import Adam
import torchvision
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable

from scipy import misc,ndimage
import multiprocessing as mp

from tqdm import tqdm
from tqdm import tqdm_notebook
from tqdm import tnrange
from tqdm.notebook import trange, tqdm

import statistics
from statistics import mean

from sklearn.model_selection import KFold
from sklearn.manifold import TSNE#import audtorch
import scipy
import datetime
from sklearn.preprocessing import normalize
#import tensorflow as tf
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from iteration_utilities import flatten
from geopy.distance import geodesic
import csv
import os
import cv2
import sys
from sklearn import metrics
import shutil
import random
import math
import PIL
import pickle
from PIL import Image
from numpy import asarray
from numpy import array
from numpy import linalg as LA
from collections import Counter
from numpy import dot
from numpy.linalg import norm
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import transforms
from functools import reduce
import json

from itertools import combinations
import matplotlib.pyplot as plt

#from keras.preprocessing.image import ImageDataGenerator,array_to_img, img_to_array, load_img

import torch.nn as nn
from torch.optim import Adam
import torchvision
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable

from scipy import misc,ndimage
import multiprocessing as mp

from tqdm import tqdm
from tqdm import tqdm_notebook
from tqdm import tnrange
from tqdm.notebook import trange, tqdm
import random
import statistics
from statistics import mean

from sklearn.model_selection import KFold
from sklearn.manifold import TSNE

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()


#------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#reading of images
def read_img(A):

    datax = []
    datay = []
    C = os.listdir(A)
    for character in C:
        images = os.listdir(A + character + '/')
        c=0
        for img in images:
          image = cv2.resize(cv2.imread(A + character + '/' + img),(224,224))
          datax.append(image)
          datay.append(character)
          c=c+1
          
          print(c)
    
    return np.array(datax), np.array(datay)
    
#-------------------------------------------------------------------------------     
def read_images(base_directory):
    """
    Reads all the alphabets from the base_directory
    Uses multithreading to decrease the reading time drastically
    """
    datax = None
    datay = None
    #pool = mp.Pool(mp.cpu_count())
    r,r1 =read_img(base_directory)
    return r,r1
#-------------------------------------------------------------------------------
#extraction of support and query images

#reading of images
def read_img1(A):

    datax = []
    Actual =[]
    images = os.listdir(A)
    c=0
    for img in images:
      Actual.append(img)
      image = cv2.resize(cv2.imread(A + img),(224,224))
      datax.append(image)
      c=c+1
    
    return np.array(datax),Actual
    
#-------------------------------------------------------------------------------     
def read_images1(base_directory):
    """
    Reads all the alphabets from the base_directory
    Uses multithreading to decrease the reading time drastically
    """
    datax = None
    datay = None
    #pool = mp.Pool(mp.cpu_count())
    r,r1=read_img1(base_directory)
    return r,r1

def extract_sample(n_way, n_support, n_query, datax, datay):
  """
  Picks random sample of size n_support+n_querry, for n_way classes
  Args:
      n_way (int): number of classes in a classification task
      n_support (int): number of labeled examples per class in the support set
      n_query (int): number of labeled examples per class in the query set
      datax (np.array): dataset of images
      datay (np.array): dataset of labels
  Returns:
      (dict) of:
        (torch.Tensor): sample of images. Size (n_way, n_support+n_query, (dim))
        (int): n_way
        (int): n_support
        (int): n_query
  """
  sample = []
  K = np.random.choice(np.unique(datay), n_way, replace=False)
  for cls in K:
    datax_cls = datax[datay == cls]
    perm = np.random.permutation(datax_cls)
    sample_cls = perm[:(n_support+n_query)]
    sample.append(sample_cls)
  sample = np.array(sample)
  sample = torch.from_numpy(sample).float()
  
  sample = sample.permute(0,1,4,2,3)
  
  return({
      'images': sample,
      'n_way': n_way,
      'n_support': n_support,
      'n_query': n_query
      })
def extract_sample1(n_way, n_support, datax, datay):
  """
  Picks random sample of size n_support+n_querry, for n_way classes
  Args:
      n_way (int): number of classes in a classification task
      n_support (int): number of labeled examples per class in the support set
      n_query (int): number of labeled examples per class in the query set
      datax (np.array): dataset of images
      datay (np.array): dataset of labels
  Returns:
      (dict) of:
        (torch.Tensor): sample of images. Size (n_way, n_support+n_query, (dim))
        (int): n_way
        (int): n_support
        (int): n_query
  """
  sample = []
  K = np.random.choice(np.unique(datay), n_way, replace=False)
  for cls in K:
    datax_cls = datax[datay == cls]
    perm = np.random.permutation(datax_cls)
    sample_cls = perm[:(n_support)]
    sample.append(sample_cls)
  sample = np.array(sample)
  sample = torch.from_numpy(sample).float()
  
  sample = sample.permute(0,1,4,2,3)
  
  return({
      'images': sample,
      'n_way': n_way,
      'n_support': n_support
      })
def extract_sample2(n_query, predictx,Actual):
  """
  Picks random sample of size n_support+n_querry, for n_way classes
  Args:
      n_way (int): number of classes in a classification task
      n_support (int): number of labeled examples per class in the support set
      n_query (int): number of labeled examples per class in the query set
      datax (np.array): dataset of images
      datay (np.array): dataset of labels
  Returns:
      (dict) of:
        (torch.Tensor): sample of images. Size (n_way, n_support+n_query, (dim))
        (int): n_way
        (int): n_support
        (int): n_query
  """
  datax_cls = predictx
  Images=Actual
  perm = np.random.permutation(Images)
  
  P = perm[:(n_query)]
  Index=[]
  for i in P:
    Ind=Actual.index(i)
    Index.append(Ind)
  sample_cls=[]
  for j in Index:
    sample_cls.append(datax_cls[j])
  sample_cls = np.array(sample_cls)
  sample_cls = torch.from_numpy(sample_cls).float()
  
  sample_cls = sample_cls.permute(0,3,1,2)
  
  return({
      'Actual_Image': P,
      'images': sample_cls,
      'n_query': n_query
      })

#-------------------------------------------------------------------------------
class Flatten(nn.Module):
  def __init__(self):
    super(Flatten, self).__init__()

  def forward(self, x):
    return x.view(x.size(0), -1)

#-------------------------------------------------------------------------------
def load_protonet_conv(**kwargs):
  """
  Loads the prototypical network model
  Arg:
      x_dim (tuple): dimension of input image
      hid_dim (int): dimension of hidden layers in conv blocks
      z_dim (int): dimension of embedded image
  Returns:
      Model (Class ProtoNet)
  """
  x_dim = kwargs['x_dim']
  hid_dim = kwargs['hid_dim']
  z_dim = kwargs['z_dim']

  def conv_block(in_channels, out_channels):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, 3, padding=1),
        nn.BatchNorm2d(out_channels),
        nn.ReLU(),
        nn.AvgPool2d(2)
        )
    
  encoder = nn.Sequential(
    conv_block(x_dim[0], hid_dim),
    conv_block(hid_dim, hid_dim),
    conv_block(hid_dim, hid_dim),
    conv_block(hid_dim, hid_dim),
    conv_block(hid_dim, hid_dim),
    conv_block(hid_dim, z_dim),
    Flatten()
    )
    
  return ProtoNet(encoder)

#-------------------------------------------------------------------------------
#formation of prototypical vectors and calculation of forward loss
class ProtoNet(nn.Module):
  def __init__(self, encoder):
    """
    Args:
        encoder : CNN encoding the images in sample
        n_way (int): number of classes in a classification task
        n_support (int): number of labeled examples per class in the support set
        n_query (int): number of labeled examples per class in the query set
    """
    super(ProtoNet, self).__init__()
    self.encoder = encoder.cuda()
  def set_forward_loss(self, sample):
    """
    Computes loss, accuracy and output for classification task
    Args:
        sample (torch.Tensor): shape (n_way, n_support+n_query, (dim)) 
    Returns:
        torch.Tensor: shape(2), loss, accuracy and y_hat
    """
    sample_images = sample['images'].cuda()
    n_way = sample['n_way']
    n_support = sample['n_support']
    n_query = sample['n_query']

    x_support = sample_images[:, :n_support]
    #print(x_support.size())
    x_query = sample_images[:, n_support:]
    #print(x_support.size())
    #target indices are 0 ... n_way-1
    target_inds = torch.arange(0, n_way).view(n_way, 1, 1).expand(n_way, n_query, 1).long()
    target_inds = Variable(target_inds, requires_grad=False)
    target_inds = target_inds.cuda()
    #print(target_inds)
    #encode images of the support and the query set
    x = torch.cat([x_support.contiguous().view(n_way * n_support, *x_support.size()[2:]),
                   x_query.contiguous().view(n_way * n_query, *x_query.size()[2:])], 0)
    #print(x.size())
    z = self.encoder.forward(x)
    #print(z.size())
    z_dim = z.size(-1) #usually 64
    #support vectors and query vectors
    
    z_samples = z[:n_way*n_support].view(n_way, n_support, z_dim)
    #print(z_samples.size())
    z_query = z[n_way*n_support:]
    
    # Select appropriate option by setting to 1
    PN_Mean = 0           # standard Mean Prototype
    PN_RRW = 0            # Robust Reweighted Protoytpe
    PN_MMD = 1           # Proposed MMD baesd Protytype

    # Find the protytype according to the selecting technique
    if PN_Mean == 1:
      z_proto = CalcluatePrototype_Mean( z_samples )
    elif PN_RRW == 1:
      z_proto = CalcluatePrototype_RRW( z_samples )
    elif PN_MMD == 1:
      z_proto = CalcluatePrototype_MMD( z_samples )

    #print(z_proto.size())
    #compute distances to prototypes
    
    dists = euclidean_dist(z_query, z_proto)
    
    
    #compute probabilities,loss and accuracy
    Soft1=F.softmax(-dists, dim=1).view(n_way, n_query, -1)
    log_p_y = F.log_softmax(-dists, dim=1).view(n_way, n_query, -1)
    
    loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()
    
    _, y_hat = log_p_y.max(2)
    
    acc_val = torch.eq(y_hat, target_inds.squeeze()).float().mean()
    #data required for AUC calculation
    R=convert1(log_p_y)
    R1=convert1(target_inds)
    R2=convert1(Soft1)
    Y_pred=convert1(y_hat)
    score=[]
    Target=[]
    score1=[]

    for i in R2:
      score1.append(list(i))
    for i in R:
      score.append(list(i))
    for i in R1:
      Target.append(int(i))
    
    
    return z_proto,z_samples,Target,z_query,Y_pred,score1,loss_val, {
        'loss': loss_val.item(),
        'acc': acc_val.item(),
        'y_hat': y_hat
        }
  def set_forward_loss1(self,sample_test,sample_predict):
      """
      Computes loss, accuracy and output for classification task
      Args:
          sample (torch.Tensor): shape (n_way, n_support+n_query, (dim)) 
      Returns:
          torch.Tensor: shape(2), loss, accuracy and y_hat
      """
      sample_images = sample_test['images'].cuda()
      sample_images1 = sample_predict['images'].cuda()
      n_way = sample_test['n_way']
      Images_actual=sample_predict['Actual_Image']
      n_support = sample_test['n_support']
      n_query = sample_predict['n_query']
      
      x_support = sample_images
      x_query = sample_images1
      
      #target indices are 0 ... n_way-1
      #target_inds = torch.arange(0, n_way).view(n_way, 1, 1).expand(n_way, n_query, 1).long()
      #target_inds = Variable(target_inds, requires_grad=False)
      #target_inds = target_inds.cuda()
      #print(target_inds)
      #encode images of the support and the query set
      #x = torch.cat([x_support.contiguous().view(n_way * n_support, *x_support.size()[2:]),x_query.contiguous().view(n_way * n_query, *x_query.size()[2:])], 0)
      z_samples = self.encoder.forward(x_support.contiguous().view(n_way * n_support, *x_support.size()[2:]))
      #print(z_samples.size())
      z_query = self.encoder.forward(x_query.contiguous())
      #print(z_query.size())
      z_dim = z_samples.size(-1) #usually 64
      #support vectors and query vectors
      
      z_samples1 = z_samples[:n_way*n_support].view(n_way, n_support, z_dim)
      #print(z_samples1.size())
      
      # Select appropriate option by setting to 1
      PN_Mean = 1           # standard Mean Prototype
      PN_RRW = 0            # Robust Reweighted Protoytpe
      PN_MMD = 0           # Proposed MMD baesd Protytype

      # Find the protytype according to the selecting technique
      if PN_Mean == 1:
        z_proto = CalcluatePrototype_Mean( z_samples1 )
      elif PN_RRW == 1:
        z_proto = CalcluatePrototype_RRW( z_samples1 )
      elif PN_MMD == 1:
        z_proto = CalcluatePrototype_MMD( z_samples1 )

      #print(z_proto.size())
      #compute distances to prototypes
      
      dists = euclidean_dist(z_query, z_proto)
      #print(dists)
      
      #compute probabilities,loss and accuracy
      Soft1=F.softmax(-dists, dim=1)
      Soft2=Soft1.cpu().data.numpy()
      return Soft2,Images_actual     
      
   

#-------------------------------------------------------------------------------
#Metric measurement-Quadratic weighted kappa
def quadratic_weighted_kappa(rater_a, rater_b, min_rating=None, max_rating=None):
    rater_a = np.array(rater_a, dtype=int)
    rater_b = np.array(rater_b, dtype=int)
    assert(len(rater_a) == len(rater_b))
    if min_rating is None:
        min_rating = min(min(rater_a), min(rater_b))
    if max_rating is None:
        max_rating = max(max(rater_a), max(rater_b))
    conf_mat = confusion_matrix(rater_a, rater_b, min_rating, max_rating)
    num_ratings = len(conf_mat)
    num_scored_items = float(len(rater_a))

    hist_rater_a = histogram(rater_a, min_rating, max_rating)
    hist_rater_b = histogram(rater_b, min_rating, max_rating)

    numerator = 0.0
    denominator = 0.0

    for i in range(num_ratings):
        for j in range(num_ratings):
            expected_count = (hist_rater_a[i] * hist_rater_b[j] / num_scored_items)
            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)
            numerator += d * conf_mat[i][j] / num_scored_items
            denominator += d * expected_count / num_scored_items
    return 1.0 - numerator / denominator
def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):
    assert(len(rater_a) == len(rater_b))
    if min_rating is None:
        min_rating = min(rater_a + rater_b)
    if max_rating is None:
        max_rating = max(rater_a + rater_b)
    num_ratings = int(max_rating - min_rating + 1)
    conf_mat = [[0 for i in range(num_ratings)]
                for j in range(num_ratings)]
    for a, b in zip(rater_a, rater_b):
        conf_mat[a - min_rating][b - min_rating] += 1
    return conf_mat


def histogram(ratings, min_rating=None, max_rating=None):
    if min_rating is None:
        min_rating = min(ratings)
    if max_rating is None:
        max_rating = max(ratings)
    num_ratings = int(max_rating - min_rating + 1)
    hist_ratings = [0 for x in range(num_ratings)]
    for r in ratings:
        hist_ratings[r - min_rating] += 1
    return hist_ratings

#function for proposed approach
def CalcluatePrototype_MMD( z_samples ):
  
  res=[]
  for i in z_samples:
    V=MMD1(i)
    res.append(V)

  #Prototypical vector formed
  z_proto = torch.stack(res)
  return z_proto

#-------------------------------------------------------------------------------
#function for existing approach
def CalcluatePrototype_Mean( z_samples ):

  res=[]
  for i in z_samples:
    sc=[1]*len(i)
    V=mean1(i,sc)
    res.append(V)
  
  z_proto = torch.stack(res)  
  return z_proto

#-------------------------------------------------------------------------------
#function for reweighting approach
def CalcluatePrototype_RRW( z_samples ):
  
  res=[]
  for i in z_samples:
    V=RRP(i)
    res.append(V)

  #Prototypical vector formed
  z_proto = torch.stack(res)
  return z_proto

#-------------------------------------------------------------------------------
#function for TSNA plots
def generate_TSNE_plots(Samp,Tar,Pro,epi,n_way,n_support):
  Pro1=Pro.cpu().data.numpy()
  target_ids=range(n_way)
  target_ids=range(n_way)
  Tar=np.array(Tar)
  Targets=[]
  for i in range(n_way):
    Targets.append(i)
  Targets=np.array(Targets)
  Samp1=[]
  for i in Samp:
    for j in i:
      Samp1.append(j.cpu().data.numpy())
  Samp1=np.array(Samp1)
  tsne = TSNE(n_components=2, init='pca')
  X_2d = tsne.fit_transform(Samp1)
  X_2d_1 = tsne.fit_transform(Pro1)
  colors = 'r', 'g', 'b'
  markers=['^','^','^']
  plt.figure(figsize=(6, 5))
  for i, c, label in zip(target_ids, colors,Targets):
    plt.scatter(X_2d[Tar == i, 0], X_2d[Tar == i, 1], c=c, label=label)
  plt.legend()
  for i, c, label,l in zip(target_ids, colors,Targets,markers):
    plt.scatter(X_2d_1[Targets == i, 0], X_2d_1[Targets == i, 1],s=500,marker=l,linewidths = 2,c=c,edgecolor ="black",label=label)
  plt.show()

#-------------------------------------------------------------------------------
#tensor gpu to cpu format
def convert2(A):
  A1=A.cpu().data.numpy()
  return A1

#-------------------------------------------------------------------------------
#tensor gpu to cpu format
def convert1(A):
  List=[]
  A1=A.cpu().data.numpy()
  for i in A1:
    for j in i:
      List.append(j)

  return List

#-------------------------------------------------------------------------------
#AUC calculation for n-way=2
def cal_AUC1(A,B,model):
  fpr, tpr, thresholds = metrics.roc_curve(np.array(B), np.array(A),pos_label=1)
  Auc=metrics.auc(fpr, tpr)
  return float(Auc)

#AUC calculation for n-way>2 for one-vs-rest
def cal_AUC3(A,B,model,n_way,n_query):
  AUC=[]
  S=0
  for i in range(n_way):
    List=[0]*len(B)
    for j in range(n_query):
      num=i*n_way+j
      List[num]=1
    fpr, tpr, thresholds = metrics.roc_curve(np.array(List), np.array(A),pos_label=1)
    Auc=metrics.auc(fpr, tpr)
    AUC.append(float(Auc))
  print("AUC values",AUC)
  print("Sum of AUC",float(sum(AUC)))
  return float(sum(AUC)/len(AUC))

#AUC calculation for n-way>2 for one-vs-one
def cal_AUC2(A,B,model,n_way,n_query):
  AUC=[]
  S=[]
  for i in range(n_way):
    S.append(i)
  for i in range(n_way):
    List=[0]*n_query*2
    List1=[0]*n_query*2
    P=S
    P.remove(i)
    L=[]
    L1=[]
    Z=[]
    Z1=[]
    for j in P:
      for k in range(n_query):
        num=i*n_query+k
        num1=j*n_query+k
        L.append(A[num])
        L1.append(1)
        Z.append(A[num1])
        Z1.append(0)
      C=L+Z
      C1=L1+Z1
      fpr, tpr, thresholds = metrics.roc_curve(np.array(C1), np.array(C),pos_label=1)
      Auc=metrics.auc(fpr, tpr)
      AUC.append(float(Auc))
  return float(sum(AUC)/len(AUC))

#-------------------------------------------------------------------------------
#MMD calculation
def MMD1(L):
  S=set()
  S1=set()
  sc=[1]*len(L)
  sc1=[1]*len(L-1)
  for i in L:
    S.add(i)
    S1.add(i)
  SC=[]
  for i in S:
    E=set()
    E.add(i)
    T=mean1(S,sc)
    T1=mean1(S1-E,sc1)
    H=torch.sub(T,T1,alpha=1)
    H1=torch.square(H)
    H2=torch.sum(H1)
    
    SC.append(H2.cpu().data.numpy())
  #normalizing MMD scores o each sample between 0 and 1              
  scores=Normalize_and_IF(SC)
  #calculation prototypical vector 
  Vec=mean1(S,scores)
  return Vec

#-------------------------------------------------------------------------------
#function to calculate weightage in reweighting scheme
def RRP(L):
  S=set()
  S1=set()
  Q=len(L)-1
  sc=[1]*Q
  for i in L:
    S.add(i)
    S1.add(i)
  SC=[]
  for i in S:
    E=set()
    E.add(i)
    A1=i.cpu().data.numpy()
    T1=mean1(S1-E,sc)

    A2=T1.cpu().data.numpy()
    PP=np.linalg.norm(A1-A2)
    SC.append(1/PP)
  #weighted prototypical vector creation
  Vec=mean1(S,SC)
  return Vec

#-------------------------------------------------------------------------------
#normalization function
def Normalize_only(L):
  List=normalize([L], norm="l1")
  List1 = [item for sublist in List for item in sublist]
  return List1

#-------------------------------------------------------------------------------
#normalization function and influence factor creator
def Normalize_and_IF(L):
  
  List=[]
  List1=[]
  List2=[]
  
  List=normalize([L], norm="l1")
  
  List1 = [item for sublist in List for item in sublist]
  
  #influence factor creation 
  for i in List1:
    List2.append(1-i)
  
  return List2

#-------------------------------------------------------------------------------
#mean calculation function 
def mean1(R,S):
  X=0
  X1=0
  for i,j in zip(R,S):
    X=X+i*j
    X1=X1+j
  E=X/X1
  
  return E

#-------------------------------------------------------------------------------
#calculation of euclidean distance
def euclidean_dist(x, y):
  """
  Computes euclidean distance btw x and y
  Args:
      x (torch.Tensor): shape (n, d). n usually n_way*n_query
      y (torch.Tensor): shape (m, d). m usually n_way
  Returns:
      torch.Tensor: shape(n, m). For each query, the distances to each centroid
  """
  n = x.size(0)
  m = y.size(0)
  d = x.size(1)
  assert d == y.size(1)
  
  x = x.unsqueeze(1).expand(n, m, d)
  y = y.unsqueeze(0).expand(n, m, d)

  return torch.pow(x - y, 2).sum(2)

#-------------------------------------------------------------------------------
def train(model, optimizer, train_x, train_y, n_way, n_support, n_query, max_epoch, epoch_size):
  """
  Trains the protonet
  Args:
      model
      optimizer
      train_x (np.array): images of training set
      train_y(np.array): labels of training set
      n_way (int): number of classes in a classification task
      n_support (int): number of labeled examples per class in the support set
      n_query (int): number of labeled examples per class in the query set
      max_epoch (int): max epochs to train on
      epoch_size (int): episodes per epoch
  """
  #divide the learning rate by 2 at each epoch, as suggested in paper
  scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)
  epoch = 0 #epochs done so far
  stop = False #status to know when to stop
  EE=datetime.datetime.now()
  count=0
  while epoch < max_epoch and not stop:
    running_loss = 0.0
    running_acc = 0.0
    for episode in tnrange(epoch_size, desc="Epoch {:d} train".format(epoch+1)):
      sample = extract_sample(n_way, n_support, n_query, train_x, train_y)
      optimizer.zero_grad()
      z_proto,z_samples,Target,z_query,Y_pred,score,loss_val,output=model.set_forward_loss(sample)
      running_loss += output['loss']
      running_acc += output['acc']
      
      loss_val.backward()
      optimizer.step()
    epoch_loss = running_loss / epoch_size
    epoch_acc = running_acc / epoch_size
    print('Epoch {:d} -- Loss: {:.4f} Acc: {:.4f}'.format(epoch+1,epoch_loss, epoch_acc))
    epoch=epoch+1
    scheduler.step()
  EE1=datetime.datetime.now()
  print('Training Time:',EE1-EE)
#-------------------------------------------------------------------------------
def test(model, test_x, test_y,predictx, n_way, n_support, n_query,uu,Actual):
  """
  Tests the protonet
  Args:
      model: trained model
      test_x (np.array): images of testing set
      test_y (np.array): labels of testing set
      n_way (int): number of classes in a classification task
      n_support (int): number of labeled examples per class in the support set
      n_query (int): number of labeled examples per class in the query set
      test_episode (int): number of episodes to test on
  """
  sample_test = extract_sample1(n_way, n_support, test_x, test_y)
  sample_predict = extract_sample2(n_query, predictx,Actual)
  Soft2,Images_actual=model.set_forward_loss1(sample_test,sample_predict)
  return Soft2,Images_actual
      
    
  
#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------
Fold=[1,2,3,4,5]
for item in Fold:
  trainx, trainy = read_images('/home/abhishek/RANJANA/Datasets/DR/Folds_'+str(item)+'/train_max_aug/')
  with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_trainx"+str(item)+".pkl","wb") as f:
    pickle.dump(trainx,f)
  with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_trainy"+str(item)+".pkl","wb") as f:
    pickle.dump(trainy,f)
testx, testy = read_images('/home/abhishek/RANJANA/Datasets/MICCAI_CHALLENGE_2022_DATASET_DR/train_max_aug_task3/')
with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_testx"+".pkl","wb") as f:
    pickle.dump(testx,f)   
with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_testy"+".pkl","wb") as f:
    pickle.dump(testy,f)
predictx,Actual=read_images1('/home/abhishek/RANJANA/MICCAI_CHALLENGE_2022_DATASET_DR/TEST/C. Diabetic Retinopathy Grading/1. Original Images/b. Testing Set/')
with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_predictx"+".pkl","wb") as f:
    pickle.dump(predictx,f) 
with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_Actualx"+".pkl","wb") as f:
    pickle.dump(Actual,f)
# Main
#-------------------------------------------------------------------------------
# Parameters
Fold=[1,2,3,4,5]
n_way = 3
n_support = 50
n_support1 = 20
n_query = 5

max_epoch =5
epoch_size = 2000

avg_acc=0
avg_auc=0
avg_kw=0
list1=[]
list2=[]
list3=[]
n=len(Fold)
start=datetime.datetime.now()
model = load_protonet_conv(x_dim=(3,224,224),hid_dim=64,z_dim=64,)
optimizer = optim.SGD(model.parameters(), lr = 0.001,momentum=0.9)
for i in Fold:
  with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_trainx"+str(i)+".pkl","rb") as f:
    trainx = pickle.load(f)
  with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_trainy"+str(i)+".pkl","rb") as f:
    trainy = pickle.load(f)
  print("*************************Training**********************************************Fold:",i)
  train(model, optimizer, trainx, trainy, n_way, n_support, n_query, max_epoch, epoch_size)

with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_testx"+".pkl","rb") as f:
  testx = pickle.load(f)
with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_testy"+".pkl","rb") as f:
  testy = pickle.load(f)
with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_predictx"+".pkl","rb") as f:
  predictx = pickle.load(f)
with open("/home/abhishek/RANJANA/Pickle_files/"+"DR_Task3_Actualx"+".pkl","rb") as f:
  Actual = pickle.load(f)
  
print("*************************Testing**********************************************Fold:")
Names=Actual
Count=[0] * len(Names)
Prob=[[0,0,0]] * len(Names)
Class=[0] * len(Names)
for episode in range(20000):
  
  Soft2,Images_actual=test(model, testx, testy, predictx,n_way, n_support1, n_query,i,Actual)

  for k,k1 in zip(Soft2,Images_actual):
    In=Actual.index(k1)
    Count[In]=Count[In]+1
    S=Prob[In]
    S1=k
    res_list = [S[i] + S1[i] for i in range(len(S))]
    Prob.insert(In,res_list)
#----------------------------------------After Testing-----------------------------------------------------------    
for i,k,l in zip(Count,Prob,range(len(Prob))):
  if i!=0:
    W = [item / i for item in k]
    Prob.insert(l,W)
for i,j in zip(Prob,range(len(Prob))):
  Max=max(i)
  Ind=i.index(Max)
  Class.insert(j,Ind)
header = ['case','class','P0', 'P1', 'P2']
with open('/home/abhishek/RANJANA/csv_files/labels.csv', 'w', encoding='UTF8', newline='') as f:
  writer = csv.writer(f)
  writer.writerow(header)
  for i,j,k in zip(Names,Class,Prob):
    Row=[]
    Row.append(i)
    Row.append(j)
    for m in k:
      Row.append(m)
    writer.writerow(Row)

